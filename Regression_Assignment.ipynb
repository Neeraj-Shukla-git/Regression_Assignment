{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Simple Linear Regression?\n",
        "\n",
        "  - Simple Linear Regression is a statistical method used to model the relationship between a dependent variable (Y) and a single independent variable (X) using a straight line. The equation is:\n",
        "  - Y = mX + c, where m is the slope and c is the intercept.\n",
        "\n",
        "2. What are the key assumptions of Simple Linear Regression?\n",
        "\n",
        "  - Linearity: The relationship between X and Y is linear.\n",
        "\n",
        "  - Independence: Observations are independent of each other.\n",
        "\n",
        "  - Homoscedasticity: Constant variance of residuals.\n",
        "\n",
        "  - Normality: Residuals are normally distributed.\n",
        "\n",
        "3. What does the coefficient m represent in the equation Y = mX + c?\n",
        "\n",
        "  - m is the slope of the line. It represents the change in the dependent variable (Y) for a one-unit increase in the independent variable (X).\n",
        "\n",
        "4. What does the intercept c represent in the equation Y = mX + c?\n",
        "\n",
        " - c is the intercept. It is the value of Y when X = 0. It shows where the regression line crosses the Y-axis.\n",
        "\n",
        "5. How do we calculate the slope m in Simple Linear Regression?\n",
        "\n",
        " - The slope m is calculated using the formula:\n",
        " - m = Σ[(X - X̄)(Y - Ȳ)] / Σ[(X - X̄)²]\n",
        "\n",
        "6. What is the purpose of the least squares method in Simple Linear Regression?\n",
        "\n",
        " - The least squares method minimizes the sum of the squares of the residuals (differences between observed and predicted values) to find the best-fitting line.\n",
        "\n",
        "7. How is the coefficient of determination (R²) interpreted in Simple Linear Regression?\n",
        "\n",
        " - R² shows how well the model explains the variability in the dependent variable.\n",
        "\n",
        " - R² = 1 means perfect fit\n",
        "\n",
        " - R² = 0 means no explanatory power\n",
        "\n",
        "8. What is Multiple Linear Regression?\n",
        "\n",
        " - Multiple Linear Regression models the relationship between a dependent variable and two or more independent variables. The equation is:\n",
        "   - Y = b₀ + b₁X₁ + b₂X₂ + ... + bₙXₙ\n",
        "\n",
        "9. What is the main difference between Simple and Multiple Linear Regression?\n",
        "\n",
        " - Simple Linear Regression has one independent variable.\n",
        "\n",
        " - Multiple Linear Regression has two or more independent variables.\n",
        "\n",
        "10. What are the key assumptions of Multiple Linear Regression?\n",
        "\n",
        " - Linearity\n",
        "\n",
        " - Independence\n",
        "\n",
        " - Homoscedasticity\n",
        "\n",
        " - Normality of residuals\n",
        "\n",
        " - No multicollinearity\n",
        "\n",
        "11. What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?\n",
        "\n",
        " - Heteroscedasticity refers to non-constant variance of residuals. It can lead to inefficient estimates and unreliable hypothesis tests.\n",
        "\n",
        "12. How can you improve a Multiple Linear Regression model with high multicollinearity?\n",
        "\n",
        " - Remove or combine correlated variables\n",
        "\n",
        " - Use Principal Component Analysis (PCA)\n",
        "\n",
        " - Use regularization techniques like Ridge or Lasso\n",
        "\n",
        "13. What are some common techniques for transforming categorical variables for use in regression models?\n",
        "\n",
        " - One-Hot Encoding\n",
        "\n",
        " - Label Encoding\n",
        "\n",
        " - Ordinal Encoding\n",
        "\n",
        "14. What is the role of interaction terms in Multiple Linear Regression?\n",
        "\n",
        " - Interaction terms allow the effect of one independent variable to depend on the level of another. They help model more complex relationships.\n",
        "\n",
        "15. How can the interpretation of intercept differ between Simple and Multiple Linear Regression?\n",
        "\n",
        " - In Simple Linear Regression, the intercept is the value of Y when X = 0.\n",
        "In Multiple Linear Regression, it’s the predicted Y when all X variables = 0, which might not always be meaningful.\n",
        "\n",
        "16. What is the significance of the slope in regression analysis, and how does it affect predictions?\n",
        "\n",
        " - The slope shows how much Y changes for each one-unit increase in X. A positive slope means a positive relationship, and vice versa.\n",
        "\n",
        "17. What are the limitations of using R² as a sole measure of model performance?\n",
        "\n",
        " - It always increases with more variables (even irrelevant ones)\n",
        "\n",
        " - It doesn’t detect overfitting\n",
        "\n",
        " - It doesn’t tell if predictors are significant\n",
        "\n",
        "18. How would you interpret a large standard error for a regression coefficient?\n",
        "\n",
        " - A large standard error means the coefficient estimate is unstable and likely not significantly different from zero.\n",
        "\n",
        "19. What is polynomial regression?\n",
        "\n",
        " - Polynomial regression is a form of regression where the relationship between the independent and dependent variable is modeled as an nth-degree polynomial.\n",
        "\n",
        "20. When is polynomial regression used?\n",
        "\n",
        " - It’s used when the relationship between variables is non-linear, but you still want to use a linear model with transformed features.\n",
        "\n",
        "21. How does the intercept in a regression model provide context for the relationship between variables?\n",
        "\n",
        " - The intercept provides the starting value of Y when all predictors are 0. It helps interpret where the line (or curve) begins.\n",
        "\n",
        "22. How can heteroscedasticity be identified in residual plots, and why is it important to address it?\n",
        "\n",
        " - In residual plots, heteroscedasticity appears as a funnel shape. It’s important to address it because it violates regression assumptions, affecting accuracy.\n",
        "\n",
        "23. What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R²?\n",
        "\n",
        " - It means that irrelevant variables are included. Adjusted R² penalizes unnecessary predictors, so the model may be overfitted.\n",
        "\n",
        "24. Why is it important to scale variables in Multiple Linear Regression?\n",
        "\n",
        " - Scaling ensures all features contribute equally and helps improve performance, especially with regularization techniques.\n",
        "\n",
        "25. How does polynomial regression differ from linear regression?\n",
        "\n",
        " - Linear regression models straight-line relationships.\n",
        " - Polynomial regression models curved (non-linear) relationships using powers of X.\n",
        "\n",
        "26. What is the general equation for polynomial regression?\n",
        "\n",
        " - Y = b₀ + b₁X + b₂X² + b₃X³ + ... + bₙXⁿ\n",
        "\n",
        "27. Can polynomial regression be applied to multiple variables?\n",
        "\n",
        " - Yes, it can. This is known as multivariate polynomial regression, where multiple features can have powers and interaction terms.\n",
        "\n",
        "28. What are the limitations of polynomial regression?\n",
        "\n",
        " - Overfitting with high degrees\n",
        "\n",
        " - Sensitive to outliers\n",
        "\n",
        " - Hard to interpret for higher-order models\n",
        "\n",
        "29. What methods can be used to evaluate model fit when selecting the degree of a polynomial?\n",
        "\n",
        " - Cross-validation\n",
        "\n",
        " - Adjusted R²\n",
        "\n",
        " - Mean Squared Error (MSE)\n",
        "\n",
        " - Visualization of residuals\n",
        "\n",
        "30. Why is visualization important in polynomial regression?\n",
        "\n",
        " - Visualization helps understand how well the model captures the pattern, detect overfitting, and interpret the curve's shape.\n",
        "\n",
        "31. How is polynomial regression implemented in Python?\n",
        "\n",
        " - Using PolynomialFeatures from sklearn.preprocessing, then fitting a LinearRegression model on the transformed features.\n"
      ],
      "metadata": {
        "id": "4yEDAqnHO3S_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "import numpy as np\n",
        "\n",
        "X = np.array([[1], [2], [3], [4]])\n",
        "y = np.array([1, 4, 9, 16])\n",
        "\n",
        "poly = PolynomialFeatures(degree=2)\n",
        "X_poly = poly.fit_transform(X)\n",
        "\n",
        "model = LinearRegression()\n",
        "model.fit(X_poly, y)\n",
        "print(\"Coefficients:\", model.coef_)\n"
      ],
      "metadata": {
        "id": "zxhw6dE9PCsV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}